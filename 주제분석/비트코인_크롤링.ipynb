{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tp9LGZ9bYQSP"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import pandas as pd\n",
        "import time\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rjZSlZsqYQSS"
      },
      "outputs": [],
      "source": [
        "mycolumns = ['date', 'press','title','content','link']\n",
        "mydata = {\n",
        "    'date' : date_list,\n",
        "    'press' : press_list,\n",
        "    'title' : title_list,\n",
        "    'content' : content_list,\n",
        "    'link' : link_list\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T54hsxNSYQST"
      },
      "outputs": [],
      "source": [
        "query = \"비트코인\"\n",
        "sort = 1\n",
        "s_date = \"2013.04.01\"\n",
        "e_date = \"2021.04.01\"\n",
        "s_from = \"20130401\"\n",
        "e_to = \"20210401\"\n",
        "\n",
        "\n",
        "mycolumns = ['date', 'press','title','content','link']\n",
        "\n",
        "num = 1\n",
        "\n",
        "while True : \n",
        "    page = 1\n",
        "    file_name = \"/Users/jaehyun/Desktop/psat_data/news_data_\" + str(num) + \".csv\"\n",
        "    \n",
        "    if num>=2 : \n",
        "        year = int(date_list[-1][0:4] + date_list[-1][5:7] + date_list[-1][8:10]) #YYYYMMDD형태\n",
        "        if year <= 20130403 : \n",
        "            break\n",
        "            \n",
        "        s_date, s_from = '2013.04.01', '20130401'\n",
        "        e_date, e_to = date_list[-1][:-1], str(year)\n",
        "        \n",
        "    title_list = [] # 제목 모음 리스트\n",
        "    content_list = [] # 내용 모음 리스트\n",
        "    link_list = [] # 링크 모음 리스트\n",
        "    date_list = [] # 날짜 모음 리스트\n",
        "    press_list = [] # 언론사 모음 리스트\n",
        "    \n",
        "    while page <= 3991 :  # 400페이지까지 반복\n",
        "        url = \"https://search.naver.com/search.naver?where=news&sm=tab_pge&query=\" + query + \"&sort=\"+str(sort)+\"photo=0&field=0&pd=3&ds=\" + s_date + \"&de=\" + e_date + \"&mynews=0&office_type=0&office_section_code=0&news_office_checked=&nso=so:dd,p:from\" + s_from + \"to\" + e_to +\",a:all&start=\" + str(page)\n",
        "        #최신순 : sort = 1, 관련도순 : sort = 0 , 오래된 순 : sort = 2\n",
        "        response = requests.get(url)\n",
        "        html = response.text\n",
        "    \n",
        "        soup = BeautifulSoup(html,'html.parser')\n",
        "        li_tmp = soup.find_all('li',{'class' : 'bx'})\n",
        "    \n",
        "        for i in li_tmp : \n",
        "            try : \n",
        "                title = i.find('a',{'class' : 'news_tit'}).text #제목\n",
        "                content = i.find('div',{'class' : 'news_dsc'}).text #내용\n",
        "                link = i.find('a',{'class' : 'news_tit'})['href'] #링크\n",
        "                press = i.find('a',{'class' : 'info press'}).text #언론사\n",
        "            \n",
        "                date_set = i.findAll('span',{'class' : 'info'})#날짜\n",
        "                if date_set : \n",
        "                    if 'A' in date_set[0].text or '면' in date_set[0].text :\n",
        "                        date = date_set[1].text\n",
        "                    else : \n",
        "                        date = date_set[0].text\n",
        "        \n",
        "                title_list.append(title)\n",
        "                content_list.append(content)\n",
        "                link_list.append(link)\n",
        "                press_list.append(press)\n",
        "                date_list.append(date)\n",
        "        \n",
        "            except : \n",
        "                continue\n",
        "    \n",
        "        page += 10\n",
        "        \n",
        "    mydata = {\n",
        "    'date' : date_list,\n",
        "    'press' : press_list,\n",
        "    'title' : title_list,\n",
        "    'content' : content_list,\n",
        "    'link' : link_list\n",
        "    }\n",
        "    \n",
        "    my_df = pd.DataFrame(data = mydata, columns = mycolumns)\n",
        "    my_df.to_csv(file_name, encoding = 'utf-8') # 파일을 나눠서 저장 - 중간에 끊길 수가 있으므로\n",
        "    \n",
        "    num += 1\n",
        "    time.sleep(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2z9vifOYQSV"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"/Users/jaehyun/Desktop/psat_data/news_data_1.csv\")\n",
        "\n",
        "for i in range(2,103) : # 저장한 파일들을 불러들이는 과정, 총 103개의 파일이 생성됨\n",
        "    file_name = \"/Users/jaehyun/Desktop/psat_data/news_data_\"+ str(i) +\".csv\"\n",
        "    concat_df = pd.read_csv(file_name)\n",
        "    df = pd.concat([df,concat_df]).drop_duplicates('content')\n",
        "    df = df.reset_index(drop = True)\n",
        "\n",
        "df = df.drop(['Unnamed: 0'],axis = 1)\n",
        "df.to_csv(\"/Users/jaehyun/Desktop/psat_data/news_data_final.csv\", encoding = 'utf-8')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "\b비트코인_크롤링.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}